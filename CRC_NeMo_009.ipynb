{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check clip sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cfg import Cfg\n",
    "C = Cfg('NIST', 16000, 'amharic', 'dev') \n",
    "\n",
    "splits=C.split_files()\n",
    "\n",
    "L=[x['t_seconds'] for x in splits]\n",
    "\n",
    "min(L),max(L),sum(L)/len(L)\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add vocoder clips to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cfg import Cfg\n",
    "C = Cfg('NIST', 16000, 'amharic', 'build') \n",
    "splits=C.split_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'NIST/openasr20_amharic/build/audio_split/BABEL_OP3_307_71263_20140602_180728_outLine_3079760_3092720.wav',\n",
       " 'key': ['BABEL',\n",
       "  'OP3',\n",
       "  '307',\n",
       "  '71263',\n",
       "  '20140602',\n",
       "  '180728',\n",
       "  'outLine',\n",
       "  '3079760',\n",
       "  '3092720'],\n",
       " 'channel': 'outLine',\n",
       " 'start': 3079760,\n",
       " 'end': 3092720,\n",
       " 't_seconds': 0.81,\n",
       " 't_begin': 192.485}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'world'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ea81b71f8a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mworld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'world'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/catskills/Desktop/openasr/Python-WORLD')\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read as wavread\n",
    "from scipy.io.wavfile import write as wavwrite\n",
    "from scipy import signal\n",
    "import librosa\n",
    "from world import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = splits[0]['name']\n",
    "\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "x,fs=librosa.load(wav_path)\n",
    "\n",
    "fs\n",
    "\n",
    "if 0:  # resample\n",
    "    fs_new = 16000\n",
    "    x = signal.resample_poly(x, fs_new, fs)\n",
    "    fs = fs_new\n",
    "\n",
    "if 0:  # low-cut\n",
    "    B = signal.firwin(127, [0.01], pass_zero=False)\n",
    "    A = np.array([1.0])\n",
    "    if 0:\n",
    "        import matplotlib.pyplot as plt\n",
    "        w, H = signal.freqz(B, A)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, figsize=(16, 6))\n",
    "        ax1.plot(w / np.pi, abs(H))\n",
    "        ax1.set_ylabel('magnitude')\n",
    "        ax2.plot(w / np.pi, np.unwrap(np.angle(H)))\n",
    "        ax2.set_ylabel('unwrapped phase')\n",
    "        plt.show()\n",
    "    x = signal.lfilter(B, A, x)\n",
    "\n",
    "vocoder = main.World()\n",
    "\n",
    "# analysis\n",
    "dat = vocoder.encode(fs, x, f0_method='harvest', is_requiem=True) # use requiem analysis and synthesis\n",
    "\n",
    "if 0:  # global pitch scaling\n",
    "    dat = vocoder.scale_pitch(dat, 1.5)\n",
    "if 0:  # global duration scaling\n",
    "    dat = vocoder.scale_duration(dat, 2)\n",
    "if 0:  # fine-grained duration modification\n",
    "    vocoder.modify_duration(dat, [1, 1.5], [0, 1, 3, -1])  # TODO: look into this\n",
    "\n",
    "\n",
    "# dat['f0'] = np.r_[np.zeros(5), dat['f0'][:-5]]\n",
    "\n",
    "for key in dat:\n",
    "    try:\n",
    "        print(key, dat[key].shape, dat[0:4])\n",
    "    except:\n",
    "        print(key, dat[key])\n",
    "\n",
    "# synthesis\n",
    "dat = vocoder.decode(dat)\n",
    "\n",
    "for key in dat:\n",
    "    try:\n",
    "        print(key, dat[key].shape)\n",
    "    except:\n",
    "        print(key, dat[key])\n",
    "\n",
    "cycles_per_frame=dat['out'].shape[0]/dat['vuv'].shape[0]\n",
    "cycles_per_frame\n",
    "\n",
    "frames_per_second=dat['fs']/cycles_per_frame\n",
    "frames_per_second\n",
    "\n",
    "if 0:  # audio\n",
    "    import simpleaudio as sa\n",
    "    snd = sa.play_buffer((dat['out'] * 2 ** 15).astype(np.int16), 1, 2, fs)\n",
    "    snd.wait_done()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "vocoder.draw(x, dat, figure_size=(12,12))\n",
    "\n",
    "wav_path[0:-4]\n",
    "\n",
    "output_fn=wav_path[0:-4]+ '-resynth.wav'\n",
    "wavwrite(output_fn, fs, (dat['out'] * 2 ** 15).astype(np.int16))\n",
    "\n",
    "output_fn\n",
    "\n",
    "import IPython\n",
    "IPython.display.Audio(wav_path)\n",
    "\n",
    "IPython.display.Audio(output_fn)\n",
    "\n",
    "from world.get_seeds_signals import get_seeds_signals\n",
    "\n",
    "ss=get_seeds_signals(dat['fs'])\n",
    "\n",
    "ss['pulse'].shape\n",
    "\n",
    "ss['noise'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step 1: Bootstrap from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cfg import Cfg\n",
    "C = Cfg('NIST', 16000, 'amharic', 'build') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_pretrained_amharic_model import load_pretrained_amharic_model\n",
    "model = load_pretrained_amharic_model(C, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os, datetime\n",
    "\n",
    "model_save_dir='save/nemo_amharic'\n",
    "\n",
    "class ModelCheckpointAtEpochEnd(pl.callbacks.ModelCheckpoint):\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        metrics['epoch'] = trainer.current_epoch\n",
    "        trainer.checkpoint_callback.on_validation_end(trainer, pl_module)\n",
    "\n",
    "pid=os.getpid()\n",
    "dt=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpointAtEpochEnd(\n",
    "    filepath=model_save_dir+'/amharic_'+f'{dt}_{pid}'+'_{epoch:02d}',\n",
    "    verbose=True,\n",
    "    save_top_k=-1,\n",
    "    save_weights_only=False,\n",
    "    period=1)\n",
    "\n",
    "trainer = pl.Trainer(gpus=[0], max_epochs=200, amp_level='O1', precision=16, checkpoint_callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "from omegaconf import DictConfig\n",
    "config_path = 'amharic_16000.yaml'\n",
    "yaml = YAML(typ='safe')\n",
    "with open(config_path) as f:\n",
    "    params = yaml.load(f)\n",
    "train_manifest=f'{C.build_dir}/train_manifest.json'\n",
    "test_manifest=f'{C.build_dir}/test_manifest.json'\n",
    "params['model']['train_ds']['manifest_filepath'] = train_manifest\n",
    "params['model']['validation_ds']['manifest_filepath'] = test_manifest\n",
    "model.set_trainer(trainer)\n",
    "model.setup_training_data(train_data_config=params['model']['train_ds'])\n",
    "model.setup_validation_data(val_data_config=params['model']['validation_ds'])\n",
    "model.setup_optimization(optim_config=DictConfig(params['model']['optim']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reshuffle_samples import reshuffle_samples\n",
    "reshuffle_samples(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply vocoder filter to DEV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cfg import Cfg\n",
    "from glob import glob\n",
    "from package_DEV import package_DEV\n",
    "from load_pretrained_amharic_model import load_pretrained_amharic_model\n",
    "version='113'\n",
    "C = Cfg('NIST', 16000, 'amharic', 'dev', version)\n",
    "model = load_pretrained_amharic_model(C, 0)\n",
    "files=list(sorted(glob(f'{C.audio_split_dir}/*.wav')))\n",
    "translations=model.transcribe(paths2audio_files=files, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from package_DEV import package_DEV\n",
    "package_DEV(C1, files, translations)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
