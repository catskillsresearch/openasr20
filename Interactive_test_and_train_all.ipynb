{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive train and test with WER all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/catskills/Desktop/openasr20/end2end_asr_pytorch')\n",
    "\n",
    "import os\n",
    "os.environ['IN_JUPYTER']='True'\n",
    "\n",
    "from chunk_size import chunk_size\n",
    "from glob import glob\n",
    "from models.asr.transformer import Transformer, Encoder, Decoder\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from utils import constant\n",
    "from utils.data_loader import SpectrogramDataset, AudioDataLoader, BucketingSampler\n",
    "from utils.functions import save_model, load_model, init_transformer_model, init_optimizer\n",
    "from utils.lstm_utils import LM\n",
    "from utils.metrics import calculate_metrics, calculate_cer, calculate_wer, calculate_cer_en_zh\n",
    "from utils.optimizer import NoamOpt\n",
    "import json, logging, math, os, random, time, torch, sys, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from Trainer import Trainer\n",
    "\n",
    "language='amharic'\n",
    "stage='NIST'\n",
    "chunks = list(sorted(glob(f'{stage}/openasr20_{language}/build/transcription_split/*.txt')))\n",
    "print(len(chunks), 'chunks')\n",
    "size_chunks=list(sorted([chunk_size(fn) for fn in chunks]))\n",
    "size_chunk_files={x:[] for x,y in size_chunks}\n",
    "for x,y in size_chunks:\n",
    "    size_chunk_files[x].append(y)\n",
    "size_chunk_distribution={x:0 for x,y in size_chunks}\n",
    "for x,y in size_chunks:\n",
    "    size_chunk_distribution[x] += 1\n",
    "\n",
    "sizes=[x for x in size_chunk_files]\n",
    "\n",
    "size_chunks[-3:]\n",
    "\n",
    "L=[]\n",
    "for n, text in reversed(sorted(size_chunks)):\n",
    "    audio=text.replace('transcription','audio').replace('txt', 'wav')\n",
    "    L.append(f'{audio},{text}')\n",
    "\n",
    "L[0]\n",
    "\n",
    "print(f'{len(L)} samples in training set')\n",
    "\n",
    "manifest_file_path=f'analysis/{language}/size_1.csv'\n",
    "with open(manifest_file_path,'w') as f:\n",
    "    f.write('\\n'.join(L))\n",
    "\n",
    "model_dir=f'save/{language}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4'\n",
    "\n",
    "args=constant.args\n",
    "args.continue_from=None\n",
    "args.cuda = True\n",
    "args.labels_path = f'analysis/{language}/{language}_characters.json'\n",
    "args.lr = 1e-4\n",
    "args.name = f'{language}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4'\n",
    "args.save_folder = f'save'\n",
    "args.epochs = 5\n",
    "args.save_every = 1\n",
    "args.feat_extractor = f'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 4\n",
    "args.num_heads = 8\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle=True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.sample_rate=8000\n",
    "args.train_manifest_list = [manifest_file_path]\n",
    "args.continue_from=f'{model_dir}/best_model.th'\n",
    "\n",
    "args.augment=True\n",
    "\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))\n",
    "\n",
    "with open(args.labels_path, 'r') as label_file:\n",
    "    labels = str(''.join(json.load(label_file)))\n",
    "\n",
    "# add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "labels = constant.PAD_CHAR + constant.SOS_CHAR + constant.EOS_CHAR + labels\n",
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "\n",
    "if constant.args.continue_from:\n",
    "        model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(\n",
    "            constant.args.continue_from)\n",
    "        start_epoch = epoch  # index starts from zero\n",
    "        verbose = constant.args.verbose\n",
    "else:\n",
    "    model = init_transformer_model(constant.args, label2id, id2label)\n",
    "    opt = init_optimizer(constant.args, model, \"noam\")\n",
    "\n",
    "start_epoch = epoch\n",
    "metrics = None\n",
    "loaded_args = None\n",
    "verbose = True\n",
    "\n",
    "constant.USE_CUDA=True\n",
    "\n",
    "train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.train_manifest_list, \n",
    "                                label2id=label2id, normalize=True, augment=args.augment)\n",
    "\n",
    "loss_type = args.loss\n",
    "model = model.cuda(0)\n",
    "num_epochs = start_epoch + 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 4\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train(model, train_loader, train_sampler, opt, loss_type, start_epoch, num_epochs, label2id, id2label, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 4\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "R = []\n",
    "\n",
    "valid_loader = train_loader\n",
    "total_valid_loss, total_valid_cer, total_valid_wer, total_valid_char, total_valid_word = 0, 0, 0, 0, 0\n",
    "for i, (data) in enumerate(valid_loader):\n",
    "    src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "    src = src.cuda()\n",
    "    tgt = tgt.cuda()\n",
    "    with autocast():\n",
    "        pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "    seq_length = pred.size(1)\n",
    "    sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "    loss, num_correct = calculate_metrics(\n",
    "        pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "    if loss.item() == float('Inf'):\n",
    "        logging.info(\"Found infinity loss, masking\")\n",
    "        loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "        continue\n",
    "\n",
    "    try: # handle case for CTC\n",
    "        strs_gold, strs_hyps = [], []\n",
    "        for ut_gold in gold_seq:\n",
    "            str_gold = \"\"\n",
    "            for x in ut_gold:\n",
    "                if int(x) == constant.PAD_TOKEN:\n",
    "                    break\n",
    "                str_gold = str_gold + id2label[int(x)]\n",
    "            strs_gold.append(str_gold)\n",
    "        for ut_hyp in hyp_seq:\n",
    "            str_hyp = \"\"\n",
    "            for x in ut_hyp:\n",
    "                if int(x) == constant.PAD_TOKEN:\n",
    "                    break\n",
    "                str_hyp = str_hyp + id2label[int(x)]\n",
    "            strs_hyps.append(str_hyp)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.info(\"NaN predictions\")\n",
    "        continue\n",
    "\n",
    "    for j in range(len(strs_hyps)):\n",
    "        strs_hyps[j] = strs_hyps[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "        strs_gold[j] = strs_gold[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "        cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "        wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "        success = 'SUCCESS' if strs_gold[j] == strs_hyps[j] else ''\n",
    "        print(f'[{j}] cer {cer} wer {wer} gold {strs_gold[j]}:{len(strs_gold[j])} hyp {strs_hyps[j]}:{len(strs_hyps[j])} {success}')\n",
    "        R.append((cer, wer, strs_gold[j], strs_hyps[j], success))\n",
    "        total_valid_cer += cer\n",
    "        total_valid_wer += wer\n",
    "        total_valid_char += len(strs_gold[j].replace(' ', ''))\n",
    "        total_valid_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "    total_valid_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL = [(x[1],y) for x,y in zip(R,L)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CER=[x[0] for x in R]\n",
    "WER=[x[1] for x in R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(CER, WER, s=1, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL=list(reversed(RL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x for x,y in RL])\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_by_wl=f'analysis/{language}/RL.json'\n",
    "with open(list_by_wl, 'w') as f:\n",
    "    json.dump(RL,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
