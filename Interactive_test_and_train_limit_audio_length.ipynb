{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive train and test audio samples limited in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9842 chunks\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9756 samples up to 12.099999999999994s, so 86 rejected\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/catskills/Desktop/openasr20/end2end_asr_pytorch')\n",
    "\n",
    "import os\n",
    "os.environ['IN_JUPYTER']='True'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "\n",
    "from audio_chunk_size import audio_chunk_size\n",
    "from glob import glob\n",
    "from models.asr.transformer import Transformer, Encoder, Decoder\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from utils import constant\n",
    "from utils.data_loader import SpectrogramDataset, AudioDataLoader, BucketingSampler\n",
    "from utils.functions import save_model, load_model, init_transformer_model, init_optimizer\n",
    "from utils.lstm_utils import LM\n",
    "from utils.metrics import calculate_metrics, calculate_cer, calculate_wer, calculate_cer_en_zh\n",
    "from utils.optimizer import NoamOpt\n",
    "import json, logging, math, os, random, time, torch, sys, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from Trainer import Trainer\n",
    "\n",
    "language='amharic'\n",
    "stage='NIST'\n",
    "chunks = list(sorted(glob(f'{stage}/openasr20_{language}/build/transcription_split/*.txt')))\n",
    "\n",
    "n_samples=len(chunks)\n",
    "print(n_samples, 'chunks')\n",
    "size_chunks=list(sorted([audio_chunk_size(fn) for fn in chunks]))\n",
    "\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "data = [x for x,y in size_chunks]\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(data, weights=np.ones(len(data)) / len(data), bins=200)\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.xlabel('Sample length (seconds)')\n",
    "plt.ylabel('Sample frequency')\n",
    "plt.title('Amharic audio sample size distribution')\n",
    "plt.savefig(f'analysis/{language}/{language}_audio_sample_size.png')\n",
    "plt.show()\n",
    "\n",
    "H=[x for x in size_chunks if x[0] <= 12.12]\n",
    "L=[]\n",
    "for length, text in H:\n",
    "    audio=text.replace('transcription','audio').replace('txt', 'wav')\n",
    "    L.append(f'{audio},{text}')\n",
    "\n",
    "n_samples\n",
    "\n",
    "print(f'{len(L)} samples up to {length}s, so {n_samples-len(L)} rejected')\n",
    "\n",
    "manifest_file_path=f'analysis/{language}/size_1.csv'\n",
    "with open(manifest_file_path,'w') as f:\n",
    "    f.write('\\n'.join(L))\n",
    "\n",
    "model_dir=f'save/{language}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4'\n",
    "\n",
    "args=constant.args\n",
    "args.continue_from=None\n",
    "args.cuda = True\n",
    "args.labels_path = f'analysis/{language}/{language}_characters.json'\n",
    "args.lr = 1e-4\n",
    "args.name = f'{language}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4'\n",
    "args.save_folder = f'save'\n",
    "args.epochs = 1000\n",
    "args.save_every = 1\n",
    "args.feat_extractor = f'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 4\n",
    "args.num_heads = 8\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle=True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.sample_rate=8000\n",
    "args.train_manifest_list = [manifest_file_path]\n",
    "args.continue_from=f'{model_dir}/best_model.th'\n",
    "\n",
    "args.augment=True\n",
    "\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))\n",
    "\n",
    "with open(args.labels_path, 'r') as label_file:\n",
    "    labels = str(''.join(json.load(label_file)))\n",
    "\n",
    "# add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "labels = constant.PAD_CHAR + constant.SOS_CHAR + constant.EOS_CHAR + labels\n",
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "\n",
    "if constant.args.continue_from:\n",
    "        model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(\n",
    "            constant.args.continue_from)\n",
    "        start_epoch = epoch  # index starts from zero\n",
    "        verbose = constant.args.verbose\n",
    "else:\n",
    "    model = init_transformer_model(constant.args, label2id, id2label)\n",
    "    opt = init_optimizer(constant.args, model, \"noam\")\n",
    "\n",
    "start_epoch = epoch\n",
    "metrics = None\n",
    "loaded_args = None\n",
    "verbose = True\n",
    "\n",
    "constant.USE_CUDA=True\n",
    "\n",
    "train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.train_manifest_list, \n",
    "                                label2id=label2id, normalize=True, augment=args.augment)\n",
    "\n",
    "loss_type = args.loss\n",
    "model = model.cuda(0)\n",
    "num_epochs = start_epoch + 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 9\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15165) TRAIN LOSS:1.4768 CER:54.88% LR:0.0000198: 100%|██████████| 1084/1084 [03:46<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15165.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15166) TRAIN LOSS:1.4737 CER:54.78% LR:0.0000198: 100%|██████████| 1084/1084 [03:46<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15166.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15167) TRAIN LOSS:1.4714 CER:54.63% LR:0.0000198: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15167.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15168) TRAIN LOSS:1.4692 CER:54.61% LR:0.0000198: 100%|██████████| 1084/1084 [03:45<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15168.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15169) TRAIN LOSS:1.4688 CER:54.65% LR:0.0000198: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15169.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15170) TRAIN LOSS:1.4658 CER:54.53% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15170.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15171) TRAIN LOSS:1.4634 CER:54.51% LR:0.0000197: 100%|██████████| 1084/1084 [03:44<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15171.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15172) TRAIN LOSS:1.4661 CER:54.56% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15172.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15173) TRAIN LOSS:1.4579 CER:54.38% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15173.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15174) TRAIN LOSS:1.4563 CER:54.30% LR:0.0000197: 100%|██████████| 1084/1084 [03:44<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15174.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15175) TRAIN LOSS:1.4550 CER:54.25% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15175.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15176) TRAIN LOSS:1.4572 CER:54.31% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15176.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15177) TRAIN LOSS:1.4516 CER:54.14% LR:0.0000197: 100%|██████████| 1084/1084 [03:42<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15177.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15178) TRAIN LOSS:1.4505 CER:54.10% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15178.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15179) TRAIN LOSS:1.4497 CER:54.17% LR:0.0000197: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15179.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15180) TRAIN LOSS:1.4490 CER:54.03% LR:0.0000196: 100%|██████████| 1084/1084 [03:42<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15180.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15181) TRAIN LOSS:1.4420 CER:54.04% LR:0.0000196: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15181.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15182) TRAIN LOSS:1.4419 CER:53.94% LR:0.0000196: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15182.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15183) TRAIN LOSS:1.4406 CER:53.96% LR:0.0000196: 100%|██████████| 1084/1084 [03:42<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15183.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15184) TRAIN LOSS:1.4375 CER:53.76% LR:0.0000196: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15184.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15185) TRAIN LOSS:1.4359 CER:53.89% LR:0.0000196: 100%|██████████| 1084/1084 [03:43<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15185.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15186) TRAIN LOSS:1.4337 CER:53.69% LR:0.0000196: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15186.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15187) TRAIN LOSS:1.4319 CER:53.66% LR:0.0000196: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15187.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15188) TRAIN LOSS:1.4327 CER:53.62% LR:0.0000196: 100%|██████████| 1084/1084 [03:42<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15188.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15189) TRAIN LOSS:1.4275 CER:53.57% LR:0.0000195: 100%|██████████| 1084/1084 [03:44<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15189.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15190) TRAIN LOSS:1.4259 CER:53.57% LR:0.0000195: 100%|██████████| 1084/1084 [03:47<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15190.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15191) TRAIN LOSS:1.4260 CER:53.44% LR:0.0000195: 100%|██████████| 1084/1084 [03:45<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15191.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15192) TRAIN LOSS:1.4211 CER:53.31% LR:0.0000195: 100%|██████████| 1084/1084 [03:42<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15192.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15193) TRAIN LOSS:1.4213 CER:53.35% LR:0.0000195: 100%|██████████| 1084/1084 [03:43<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15193.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15194) TRAIN LOSS:1.4197 CER:53.42% LR:0.0000195: 100%|██████████| 1084/1084 [03:47<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/epoch_15194.th\n",
      "SAVE MODEL to save/amharic_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4/best_model.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15195) TRAIN LOSS:1.4120 CER:52.92% LR:0.0000195:  99%|█████████▉| 1073/1084 [03:44<00:02,  4.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d27667d2ef3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/openasr20/Trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_loader, train_sampler, opt, loss_type, start_epoch, num_epochs, label2id, id2label, last_metrics)\u001b[0m\n\u001b[1;32m     63\u001b[0m                             \u001b[0mstr_gold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mut_gold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                 \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAD_TOKEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                 \u001b[0mstr_gold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_gold\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(model, train_loader, train_sampler, opt, loss_type, start_epoch, num_epochs, label2id, id2label, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 8\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = constant.args.label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R = []\n",
    "\n",
    "valid_loader = train_loader\n",
    "total_valid_loss, total_valid_cer, total_valid_wer, total_valid_char, total_valid_word = 0, 0, 0, 0, 0\n",
    "for i, (data) in enumerate(valid_loader):\n",
    "    src, tgt, src_percentages, src_lengths, tgt_lengths = data\n",
    "    src = src.cuda()\n",
    "    tgt = tgt.cuda()\n",
    "    with autocast():\n",
    "        pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "\n",
    "    seq_length = pred.size(1)\n",
    "    sizes = Variable(src_percentages.mul_(int(seq_length)).int(), requires_grad=False)\n",
    "\n",
    "    loss, num_correct = calculate_metrics(\n",
    "        pred, gold, input_lengths=sizes, target_lengths=tgt_lengths, smoothing=smoothing, loss_type=loss_type)\n",
    "\n",
    "    if loss.item() == float('Inf'):\n",
    "        logging.info(\"Found infinity loss, masking\")\n",
    "        loss = torch.where(loss != loss, torch.zeros_like(loss), loss) # NaN masking\n",
    "        continue\n",
    "\n",
    "    try: # handle case for CTC\n",
    "        strs_gold, strs_hyps = [], []\n",
    "        for ut_gold in gold_seq:\n",
    "            str_gold = \"\"\n",
    "            for x in ut_gold:\n",
    "                if int(x) == constant.PAD_TOKEN:\n",
    "                    break\n",
    "                str_gold = str_gold + id2label[int(x)]\n",
    "            strs_gold.append(str_gold)\n",
    "        for ut_hyp in hyp_seq:\n",
    "            str_hyp = \"\"\n",
    "            for x in ut_hyp:\n",
    "                if int(x) == constant.PAD_TOKEN:\n",
    "                    break\n",
    "                str_hyp = str_hyp + id2label[int(x)]\n",
    "            strs_hyps.append(str_hyp)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logging.info(\"NaN predictions\")\n",
    "        continue\n",
    "\n",
    "    for j in range(len(strs_hyps)):\n",
    "        strs_hyps[j] = strs_hyps[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "        strs_gold[j] = strs_gold[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "        cer = calculate_cer(strs_hyps[j].replace(' ', ''), strs_gold[j].replace(' ', ''))\n",
    "        wer = calculate_wer(strs_hyps[j], strs_gold[j])\n",
    "        success = 'SUCCESS' if strs_gold[j] == strs_hyps[j] else ''\n",
    "        print(f'[{j}] cer {cer} wer {wer} gold {strs_gold[j]}:{len(strs_gold[j])} hyp {strs_hyps[j]}:{len(strs_hyps[j])} {success}')\n",
    "        R.append((cer, wer, strs_gold[j], strs_hyps[j], success))\n",
    "        total_valid_cer += cer\n",
    "        total_valid_wer += wer\n",
    "        total_valid_char += len(strs_gold[j].replace(' ', ''))\n",
    "        total_valid_word += len(strs_gold[j].split(\" \"))\n",
    "\n",
    "    total_valid_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in R if x[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4706-3339)/4706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([x[1] for x in R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(x[2].split(' ')) for x in R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2178/10483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
