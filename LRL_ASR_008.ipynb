{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Resource Language ASR model\n",
    "### Lars Ericson, Catskills Research Company, OpenASR20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cfg import Cfg\n",
    "C = Cfg('NIST', 8000, 'amharic') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RecordingCorpus import RecordingCorpus\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(16) as pool:\n",
    "        recordings = RecordingCorpus(C, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recordings.sample_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings.visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings.artifacts[0].display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SplitCorpus import SplitCorpus\n",
    "\n",
    "fat=SplitCorpus.from_recordings(C, recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat.sample_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split corpus down to 1 or best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SubSplitCorpus import SubSplitCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsplits=SubSplitCorpus(fat, min_words=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out why I'm still seeing a 11.98 second clip and fix that and redo subsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ones=[x for x in subsplits.artifacts if x.source.n_seconds >=  16]\n",
    "len(long_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact=long_ones[0]\n",
    "artifact.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subsplits.problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bfgpu.pkl', 'wb') as f:\n",
    "    pickle.dump(subsplits,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l bfgpu.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if I'm losing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=fat.check_vocabulary_change(subsplits)\n",
    "len(dp)\n",
    "print(f'lost {len(dp)} words' if len(dp) else 'no lost words!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check I'm not losing graphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_stats = fat.sample_statistics()\n",
    "sub_stats=subsplits.sample_statistics()\n",
    "\n",
    "fat_stats[fat_stats.Units==\"#Graphemes\"].Value==sub_stats[sub_stats.Units==\"#Graphemes\"].Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix problems in sample statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(artifact.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo=[x for x in subsplits.artifacts if x.source.n_seconds > 4]\n",
    "len(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "370/8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArtifactsVector import ArtifactsVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=subsplits.artifacts\n",
    "good=[x for x in subsplits.artifacts if x.source.n_samples > 370]\n",
    "bad=[x for x in subsplits.artifacts if x.source.n_samples <=370]\n",
    "len(good), len(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsplits.artifacts=good\n",
    "subsplits.problems=bad\n",
    "subsplits.population=ArtifactsVector(C, subsplits.artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsplits.sample_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix problems in visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subsplits.visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show change in sample stats and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat.diff_sample_statistics(subsplits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fat.diff_visualization(subsplits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a test training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/rm -rf frob\n",
    "!mkdir -p frob/audio\n",
    "!mkdir -p frob/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "clips=[]\n",
    "for i, artifact in enumerate(tqdm(subsplits.artifacts)):\n",
    "    sound = artifact.source.value\n",
    "    fn=f\"frob/audio/clip_{i}.wav\"\n",
    "    sf.write(fn, sound, C.sample_rate)\n",
    "    clips.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for i, artifact in enumerate(tqdm(subsplits.artifacts)):\n",
    "    text = artifact.target.value\n",
    "    fn=f\"frob/text/text_{i}.txt\"\n",
    "    with open(fn,'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    texts.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_fn=\"frob/manifest.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(manifest_fn, 'w') as f:\n",
    "    f.write('\\n'.join([f'{a},{b}' for a,b in zip(clips, texts)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASR end-to-end speech-to-grapheme model stacked on top of grapheme-to-grapheme corrector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.extension='_gradscaler'\n",
    "C.batch_size=12\n",
    "C.save_every = 1\n",
    "C.start_from = 246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys, os, librosa, random, math, time, torch\n",
    "sys.path.append('/home/catskills/Desktop/openasr20/end2end_asr_pytorch')\n",
    "os.environ['IN_JUPYTER']='True'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import soundfile as sf\n",
    "from utils import constant\n",
    "from utils.functions import load_model\n",
    "from utils.data_loader import SpectrogramDataset, AudioDataLoader, BucketingSampler\n",
    "from clip_ends import clip_ends\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data import TabularDataset\n",
    "import matplotlib.ticker as ticker\n",
    "from IPython.display import Audio\n",
    "from unidecode import unidecode\n",
    "from seq_to_seq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.analysis_dir = f'analysis/{C.language}'\n",
    "C.grapheme_dictionary_fn = f'{C.analysis_dir}/{C.language}_characters.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.model_name=f'{C.language}_{C.sample_rate}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4{C.extension}'\n",
    "C.model_dir=f'save/{C.model_name}'\n",
    "C.best_model=f'{C.model_dir}/best_model.th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=constant.args\n",
    "args.continue_from=None\n",
    "args.cuda = True\n",
    "args.labels_path = C.grapheme_dictionary_fn\n",
    "args.lr = 1e-4\n",
    "args.name = C.model_name\n",
    "args.save_folder = f'save'\n",
    "args.epochs = 1000\n",
    "args.save_every = 1\n",
    "args.feat_extractor = f'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 4\n",
    "args.num_heads = 8\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle=True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.sample_rate=C.sample_rate\n",
    "args.continue_from=C.best_model\n",
    "args.augment=True\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.labels_path, 'r') as label_file:\n",
    "    labels = str(''.join(json.load(label_file)))\n",
    "# add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "labels = constant.PAD_CHAR + constant.SOS_CHAR + constant.EOS_CHAR + labels\n",
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "\n",
    "model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(constant.args.continue_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant.USE_CUDA=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "loaded_args = None\n",
    "loss_type = args.loss\n",
    "\n",
    "if constant.USE_CUDA:\n",
    "    model = model.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = constant.args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradually skim the cream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrainerVanilla import TrainerVanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grab_text import grab_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for zzz in range(100):\n",
    "    manifest_fn=\"cream.csv\"\n",
    "    args.train_manifest_list=[manifest_fn]\n",
    "    train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.train_manifest_list, label2id=label2id, normalize=True, augment=args.augment)\n",
    "    train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "    train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "    trainer = TrainerVanilla()\n",
    "    sample_results = trainer.train(model, train_loader, train_sampler, opt, loss_type, \n",
    "                                   start_epoch, num_epochs, label2id, id2label, just_once=True)\n",
    "    n_samples=len(sample_results)\n",
    "    print('n_samples', n_samples)\n",
    "\n",
    "    performance=pd.DataFrame(sample_results, columns=['Pred', 'Gold', 'CER', 'WER'])\n",
    "    performance['GOLD_n_chars']=performance['Gold'].str.len()\n",
    "    performance['CER_pct']=performance['CER']/performance['GOLD_n_chars']\n",
    "    performance['GOLD_n_words']=performance['Gold'].apply(lambda x: len(x.split(' ')))\n",
    "    performance['WER_pct']=performance['WER']/performance['GOLD_n_words']\n",
    "    performance.CER_pct.hist(bins=100);\n",
    "    performance.plot(kind='scatter', x='GOLD_n_words', y='CER_pct');\n",
    "    threshold=max(0.1, threshold*0.95)\n",
    "    print(\"threshold\", threshold)\n",
    "    dft=performance[performance.CER_pct < threshold]\n",
    "    n_threshold=len(dft)\n",
    "    display(dft.head())\n",
    "    print('n_threshold',n_threshold)\n",
    "    print('n_threshold/n_samples', n_threshold/n_samples)\n",
    "    gold=performance[performance.CER_pct < threshold].Gold.values\n",
    "    with open(manifest_fn, 'r') as f:\n",
    "        manifest=f.readlines()\n",
    "    M1=[[y.strip() for y in x.split(',')] for x in manifest]\n",
    "    M2={grab_text(txtfn):(wavfn,txtfn) for wavfn, txtfn in M1}\n",
    "    cream=[M2[x] for x in gold]\n",
    "    with open('cream.csv', 'w') as f:\n",
    "        for a,b in cream:\n",
    "            f.write(f'{a},{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", 1000): \n",
    "    display(performance.sort_values(\"GOLD_n_chars\", ascending=False).head(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afterburner training to correct piece-by-piece quasi-phonemic (\"learned phonemic\") translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather training data from a rerun on all subsplits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_fn=\"frob/manifest.csv\"\n",
    "args.train_manifest_list=[manifest_fn]\n",
    "train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=args.train_manifest_list, label2id=label2id, normalize=True, augment=args.augment)\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "trainer = TrainerVanilla()\n",
    "sample_results = trainer.train(model, train_loader, train_sampler, opt, loss_type, \n",
    "                               start_epoch, num_epochs, label2id, id2label, just_once=True)\n",
    "n_samples=len(sample_results)\n",
    "print('n_samples', n_samples)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('sample_results.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format training data TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_correction_training_fn='error_correction.tsv'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training='\\n'.join([f'{gold.strip()}\\t{pred.strip()}' for pred,gold,z,w in sample_results])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(error_correction_training_fn, 'w', encoding='utf-8') as f:\n",
    "    f.write(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(error_correction_training_fn, 'r', encoding='utf-8') as f:\n",
    "    training=f.read()\n",
    "GP=training.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphemes=list(sorted(set([x for x in training if x not in ['\\n', '\\t']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP=[x.split('\\t') for x in GP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=max([max(len(a), len(b)) for a,b in GP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls error_correction.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "tokenize=lambda x: [y for y in x]\n",
    "\n",
    "SRC = Field(tokenize = tokenize, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "train_data = TabularDataset(\n",
    "    path=error_correction_training_fn,\n",
    "    format='tsv',\n",
    "    fields=[('trg', TRG), ('src', SRC)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = Iterator(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(graphemes, min_freq = MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(graphemes, min_freq = MIN_FREQ)\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "INPUT_DIM, OUTPUT_DIM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq_to_seq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device,\n",
    "              MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device,\n",
    "              MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn='tut6-model.pt'\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights);\n",
    "if os.path.exists(model_fn):\n",
    "    model.load_state_dict(torch.load(model_fn))\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(1):\n",
    "    epoch_loss = 0\n",
    "    print(f\"# batches: {len(train_iterator)}\")\n",
    "    for i, batch in enumerate(train_iterator):\n",
    "\n",
    "        print(f\"start step {j} {i} src max {batch.src.max().item} trg max {batch.trg.max().item()}\")\n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(f'[{i}] {epoch_loss}')\n",
    "        \n",
    "    print(j, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=output.argmax(1).cpu().detach().numpy()\n",
    "''.join([SRC.vocab.itos[x] for x in src.cpu().numpy()[0]])\n",
    "silver=''.join([TRG.vocab.itos[x] for x in trg.cpu().numpy()]).split('<eos>')[0]\n",
    "pred=''.join([TRG.vocab.itos[x] for x in pred]).split('<eos>')[0]\n",
    "from utils.metrics import calculate_cer, calculate_wer\n",
    "calculate_cer(pred, silver)\n",
    "calculate_wer(pred, silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate BUILD a whole split at a time, using SubSplit splitting method, and correct that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate BUILD a whole recording at a time, using SubSplit splitting method, and correct that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
