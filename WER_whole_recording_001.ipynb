{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WER on single recording end to end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pickle, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from graphviz import Digraph\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "os.environ['IN_JUPYTER']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amharic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cfg import Cfg\n",
    "C = Cfg('NIST', 8000, 'amharic') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Corpus into Recordings $\\langle A_i,T_i \\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import split_corpus_into_recordings\n",
    "split_corpus_into_recordings.diagram(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from RecordingCorpus import RecordingCorpus\n",
    "from multiprocessing import Pool\n",
    "from contextlib import closing\n",
    "if __name__ == '__main__':\n",
    "    with closing(Pool(16)) as pool:\n",
    "        recordings = RecordingCorpus(C, pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Recordings $\\langle A_i,T_i \\rangle$ into Splits $\\langle A_{i,j}, T_{i,j} \\rangle$ following transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import split_recordings_into_splits\n",
    "split_recordings_into_splits.diagram(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SplitCorpus import SplitCorpus\n",
    "splits=SplitCorpus.transcript_split(C, recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_splits={x.key:x for x in splits.artifacts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsplit $A_{i,j}$ on silence and apportion text to trimmed chunks by word sizes in $T_{i,j} = \\langle w_{i,j,k} \\rangle$ where $w_{i,j,k}=\\langle g_{i,j,k,l} \\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subsplit_Aij_on_silence_and_apportion_text_by_size\n",
    "subsplit_Aij_on_silence_and_apportion_text_by_size.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SubSplitCorpus import SubSplitCorpus\n",
    "if __name__==\"__main__\":\n",
    "    if False:\n",
    "        with closing(Pool(16)) as pool:\n",
    "            subsplits=SubSplitCorpus(pool, splits, min_words=2)\n",
    "        with open('bfgpu.pkl', 'wb') as f:\n",
    "            pickle.dump(subsplits,f)\n",
    "    else:\n",
    "        with open('bfgpu.pkl', 'rb') as f:\n",
    "            subsplits=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_subsplits={x.key:x for x in subsplits.artifacts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ASR NN on subsplits $\\langle A_{i,j,k}, T_{i,j,k} \\rangle$ inferring $P_{i,j,k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_ASR_NN_on_subsplits\n",
    "train_ASR_NN_on_subsplits.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['IN_JUPYTER']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASR_NN import ASR_NN\n",
    "model_ASR = ASR_NN(C);\n",
    "model_ASR.load_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts=subsplits.artifacts\n",
    "subsplits.artifacts=artifacts[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ASR.load_training_set(subsplits, batch_size=8)\n",
    "#ASR_output = model_ASR.infer()\n",
    "ASR_output = model_ASR.train()\n",
    "ASR_scores = model_ASR.score(ASR_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up GPU memory for next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "del model_ASR\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate $P_{i,j,k}$ giving $P_{i,j} = P_{i,j,1}\\|\\cdots\\|P_{i,j,-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concatenate_Pikj_giving_Pij\n",
    "concatenate_Pikj_giving_Pij.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order, hyp in ASR_scores[['order','hyp']].values:\n",
    "    subsplits.artifacts[order].P = hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pij={x.key[0:-1]:{} for x in subsplits.artifacts}\n",
    "\n",
    "for x in subsplits.artifacts:\n",
    "    Pij[x.key[0:-1]][x.key[-1]] = x.P\n",
    "\n",
    "for key in Pij:\n",
    "    Pij[key] = ' '.join([y for x,y in Pij[key].items()])\n",
    "\n",
    "for key in Pij:\n",
    "    id_splits[key].P=Pij[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join text corrector training sets $\\langle P_{i,j,k}, T_{i,j,k}\\rangle$ and $\\langle P_{i,j}, T_{i,j}\\rangle$ giving training set $\\langle P, T\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import join_Pijk_and_Pij_giving_P\n",
    "join_Pijk_and_Pij_giving_P.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pijk_Tijk=[(x.P, x.target.value, x) for x in subsplits.artifacts]\n",
    "In [ ]:\n",
    "￼\n",
    "import score_TiQi_using_sclite_on_all_recordings_together\n",
    "score_TiQi_using_sclite_on_all_recordings_together.diagram\n",
    "In [ ]:\n",
    "￼\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pij_Tij=[(y.P, y.target.value, y) for x,y in id_splits.items() if 'P' in vars(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_T=Pijk_Tijk+Pij_Tij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l pt.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open('pt.pkl', 'wb') as f:\n",
    "        pickle.dump(P_T, f)\n",
    "else:\n",
    "    with open('pt.pkl', 'rb') as f:\n",
    "        P_T = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer text-to-text corrector $P\\to Q\\approx T$ and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import train_text_to_text_corrector_on_PQT_score\n",
    "train_text_to_text_corrector_on_PQT_score.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTC_NN import TTC_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_TTC = TTC_NN(C);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_TTC.load_training_set(P_T, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_TTC.load_model('save/afterburner/afterburner_002_96473_1437.pt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_TTC.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTC_output = model_TTC.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_TTC.train_data), len(TTC_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTC_scores = model_TTC.score(TTC_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTC_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_TTC\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsplit $A_{i,j}$ on silence alone retaining timecode $(s_{i,j,k},e_{i,j,k})$ giving $A_{i,j} = \\langle (\\alpha_{i,j,k}, (s_{i,j,k},e_{i,j,k})) \\rangle$ so that $\\alpha_{i,j,k} = A_{i,j}[s_{i,j,k}:e_{i,j,k}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subsplit_Aij_on_silence_retaining_timecode\n",
    "subsplit_Aij_on_silence_retaining_timecode.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SplitCorpus import SplitCorpus\n",
    "ssplits=SplitCorpus.split_on_silence(C, recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ssplits.pkl', 'wb') as f:\n",
    "    pickle.dump(ssplits,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer $(\\alpha_{i,j,k}, (s_{i,j,k},e_{i,j,k})) \\xrightarrow{ASR} p_{i,j,k} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import infer_aijk_giving_pijk\n",
    "infer_aijk_giving_pijk.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ssplits.pkl', 'rb') as f:\n",
    "    ssplits=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASR_NN import ASR_NN\n",
    "model_ASR = ASR_NN(C);\n",
    "model_ASR.load_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ASR.load_training_set(ssplits, batch_size=24)\n",
    "ASR_output = model_ASR.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bysize=list(reversed(sorted([(x.source.n_seconds,x) for x in ssplits.artifacts], key=lambda x: x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bysize[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "bysize[10][1].display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts=ssplits.artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssplits.artifacts=[bysize[10][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ASR.load_training_set(ssplits, batch_size=24)\n",
    "ASR_output = model_ASR.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASR_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate $p_{i,j,k} \\to p_{i,j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concatenate_pikj_giving_pij\n",
    "concatenate_pikj_giving_pij.diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer $p_{i,j} \\xrightarrow{TT} (q_{i,j},(s_{i,j,0},e_{i,j,-1}))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import infer_pij_giving_qij\n",
    "infer_pij_giving_qij.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate $\\langle \\langle (q_{i,j,k},(s_{i,j,k},e_{i,j,k})) \\rangle \\rangle $ giving transcript $Q_i = \\langle (q_{i,j}, (s_{i,j,0},e_{i,j,-1})) \\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concatenate_qijk_giving_Qi\n",
    "concatenate_qijk_giving_Qi.diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transcript $Q_i$ in .CTM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import save_transcript_Qi_in_CTM_format\n",
    "save_transcript_Qi_in_CTM_format.diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score $\\langle T_i,Q_i\\rangle$ using `sclite` for each recording individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score_TiQi_using_sclite_per_recording\n",
    "score_TiQi_using_sclite_per_recording.diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score $\\langle T_i,Q_i\\rangle$ using `sclite` collectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score_TiQi_using_sclite_on_all_recordings_together\n",
    "score_TiQi_using_sclite_on_all_recordings_together.diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
