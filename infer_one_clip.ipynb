{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer one clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/catskills/Desktop/openasr20/end2end_asr_pytorch')\n",
    "\n",
    "import os\n",
    "os.environ['IN_JUPYTER']='True'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from utils import constant\n",
    "import json\n",
    "from utils.functions import load_model\n",
    "\n",
    "language='amharic'\n",
    "stage='NIST'\n",
    "\n",
    "model_dir=f'save/{language}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4'\n",
    "args=constant.args\n",
    "args.continue_from=None\n",
    "args.cuda = True\n",
    "args.labels_path = f'analysis/{language}/{language}_characters.json'\n",
    "args.lr = 1e-4\n",
    "args.name = f'{language}_end2end_asr_pytorch_drop0.1_cnn_batch12_4_vgg_layer4'\n",
    "args.save_folder = f'save'\n",
    "args.epochs = 1000\n",
    "args.save_every = 1\n",
    "args.feat_extractor = f'vgg_cnn'\n",
    "args.dropout = 0.1\n",
    "args.num_layers = 4\n",
    "args.num_heads = 8\n",
    "args.dim_model = 512\n",
    "args.dim_key = 64\n",
    "args.dim_value = 64\n",
    "args.dim_input = 161\n",
    "args.dim_inner = 2048\n",
    "args.dim_emb = 512\n",
    "args.shuffle=True\n",
    "args.min_lr = 1e-6\n",
    "args.k_lr = 1\n",
    "args.sample_rate=8000\n",
    "args.continue_from=f'{model_dir}/best_model.th'\n",
    "args.augment=True\n",
    "audio_conf = dict(sample_rate=args.sample_rate,\n",
    "                  window_size=args.window_size,\n",
    "                  window_stride=args.window_stride,\n",
    "                  window=args.window,\n",
    "                  noise_dir=args.noise_dir,\n",
    "                  noise_prob=args.noise_prob,\n",
    "                  noise_levels=(args.noise_min, args.noise_max))\n",
    "\n",
    "with open(args.labels_path, 'r') as label_file:\n",
    "    labels = str(''.join(json.load(label_file)))\n",
    "# add PAD_CHAR, SOS_CHAR, EOS_CHAR\n",
    "labels = constant.PAD_CHAR + constant.SOS_CHAR + constant.EOS_CHAR + labels\n",
    "label2id, id2label = {}, {}\n",
    "count = 0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] not in label2id:\n",
    "        label2id[labels[i]] = count\n",
    "        id2label[count] = labels[i]\n",
    "        count += 1\n",
    "    else:\n",
    "        print(\"multiple label: \", labels[i])\n",
    "\n",
    "model, opt, epoch, metrics, loaded_args, label2id, id2label = load_model(constant.args.continue_from)\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_loader import SpectrogramDataset, AudioDataLoader, BucketingSampler\n",
    "\n",
    "audio_fns=list(sorted(glob('NIST/openasr20_amharic/build/audio_split_normalized_to_18db/BABEL_OP3_307_14229_20140503_233516_inLine_*.wav')))\n",
    "text='infer.txt'\n",
    "manifest_fn='manifest.csv'\n",
    "manifest='\\n'.join([f'{audio},{text}' for audio in audio_fns])\n",
    "with open(manifest_fn, 'w') as f:\n",
    "    f.write(manifest)\n",
    "\n",
    "train_data = SpectrogramDataset(audio_conf, manifest_filepath_list=[manifest_fn], \n",
    "                                label2id=label2id, normalize=True, augment=args.augment)\n",
    "args.batch_size=1\n",
    "train_sampler = BucketingSampler(train_data, batch_size=args.batch_size)\n",
    "train_loader = AudioDataLoader(train_data, num_workers=args.num_workers, batch_sampler=train_sampler)\n",
    "\n",
    "strs_hyps=[]\n",
    "for i, (data) in enumerate(tqdm(train_loader)):\n",
    "    src, tgt, _, src_lengths, tgt_lengths = data\n",
    "    src = src.cuda()\n",
    "    tgt = tgt.cuda()\n",
    "    pred, gold, hyp_seq, gold_seq = model(src, src_lengths, tgt, verbose=False)\n",
    "    seq_length = pred.size(1)\n",
    "    for ut_hyp in hyp_seq:\n",
    "        str_hyp = \"\"\n",
    "        for x in ut_hyp:\n",
    "            if int(x) == constant.PAD_TOKEN:\n",
    "                break\n",
    "            str_hyp = str_hyp + id2label[int(x)]\n",
    "        strs_hyps.append(str_hyp)\n",
    "for j in range(len(strs_hyps)):\n",
    "    strs_hyps[j] = strs_hyps[j].replace(constant.SOS_CHAR, '').replace(constant.EOS_CHAR, '')\n",
    "\n",
    "len(R), len(strs_hyps)\n",
    "\n",
    "with open(f'analysis/{language}/TEST_BUILD.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(strs_hyps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
