{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  8 03:07 amharic\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 14:21 cantonese\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 14:23 guarani\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 23:09 javanese\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 14:22 kurmanji-kurdish\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 14:22 mongolian\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 23:09 pashto\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  8 09:06 somali\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 14:22 tamil\r\n",
      "drwxrwxr-x 2 catskills catskills 4096 Sep  7 14:22 vietnamese\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "language='tamil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=glob(f'save/{language}*/b*.th')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo=torch.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'¶': 0,\n",
       " '§': 1,\n",
       " '¤': 2,\n",
       " 'க': 3,\n",
       " '-': 4,\n",
       " 'அ': 5,\n",
       " 'ா': 6,\n",
       " 'உ': 7,\n",
       " 'ஜ': 8,\n",
       " 'ை': 9,\n",
       " 'ன': 10,\n",
       " 'ே': 11,\n",
       " 'வ': 12,\n",
       " 'ீ': 13,\n",
       " 'ஷ': 14,\n",
       " 'ண': 15,\n",
       " '்': 16,\n",
       " 'ோ': 17,\n",
       " 'ஒ': 18,\n",
       " 'ு': 19,\n",
       " 'ௌ': 20,\n",
       " 'ம': 21,\n",
       " 'ச': 22,\n",
       " 'ஞ': 23,\n",
       " 'எ': 24,\n",
       " 'ப': 25,\n",
       " 'ய': 26,\n",
       " 'ொ': 27,\n",
       " 'த': 28,\n",
       " 'ெ': 29,\n",
       " 'ஓ': 30,\n",
       " 'ஏ': 31,\n",
       " 'ற': 32,\n",
       " ' ': 33,\n",
       " 'ஊ': 34,\n",
       " 'ங': 35,\n",
       " 'ஶ': 36,\n",
       " 'ல': 37,\n",
       " 'ட': 38,\n",
       " 'ஸ': 39,\n",
       " 'ஐ': 40,\n",
       " 'ஆ': 41,\n",
       " 'ள': 42,\n",
       " 'ஹ': 43,\n",
       " 'ஈ': 44,\n",
       " 'ஃ': 45,\n",
       " 'ர': 46,\n",
       " 'ூ': 47,\n",
       " 'ழ': 48,\n",
       " 'இ': 49,\n",
       " 'ி': 50,\n",
       " 'ந': 51}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['label2id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcf=glob(f'analy*/{language}/*char*.json')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gcf, 'r') as f:\n",
    "    chars=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in chars if x not in foo['label2id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¶', '§', '¤']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in foo['label2id']  if x not in chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How compressed are the layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "import zlib\n",
    "\n",
    "def compression_ratio_np(A):\n",
    "    B = zlib.compress(A, 1)\n",
    "    BZ = zlib.compress(A,0)\n",
    "    return len(B)/len(BZ)\n",
    "\n",
    "def compression_ratio(T):\n",
    "    return compression_ratio_np(T.cpu().numpy())\n",
    "\n",
    "def byte_size(a):\n",
    "    return a.element_size() * a.nelement()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=foo['model_state_dict']\n",
    "rows=[]\n",
    "for x in D:\n",
    "    a=D[x]\n",
    "    rows.append([x, a.shape, byte_size(a), compression_ratio(a)])\n",
    "df_compression=pd.DataFrame(rows, columns=['Parameter', 'Shape', 'Size', 'Compression Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Size</th>\n",
       "      <th>Compression Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>encoder.input_linear.weight</td>\n",
       "      <td>(512, 5120)</td>\n",
       "      <td>10485760</td>\n",
       "      <td>0.930760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encoder.input_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.950461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encoder.layer_norm_input.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.837785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encoder.layer_norm_input.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.926178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encoder.positional_encoding.pe</td>\n",
       "      <td>(1, 4000, 512)</td>\n",
       "      <td>8192000</td>\n",
       "      <td>0.914195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>encoder.layers.0.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>encoder.layers.0.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>encoder.layers.0.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encoder.layers.0.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>encoder.layers.0.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>encoder.layers.0.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>encoder.layers.0.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.830015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>encoder.layers.0.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>encoder.layers.0.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>encoder.layers.0.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>encoder.layers.0.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>encoder.layers.0.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.929416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>encoder.layers.0.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.928755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>encoder.layers.0.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>encoder.layers.0.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.838757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>encoder.layers.0.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>encoder.layers.1.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>encoder.layers.1.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>encoder.layers.1.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>encoder.layers.1.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>encoder.layers.1.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>encoder.layers.1.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.942205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>encoder.layers.1.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.842156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>encoder.layers.1.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>encoder.layers.1.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>encoder.layers.1.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>encoder.layers.1.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>encoder.layers.1.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.929172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>encoder.layers.1.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>encoder.layers.1.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>encoder.layers.1.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.843613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>encoder.layers.1.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>encoder.layers.2.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>encoder.layers.2.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>encoder.layers.2.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>encoder.layers.2.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>encoder.layers.2.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>encoder.layers.2.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>encoder.layers.2.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.852841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>encoder.layers.2.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>encoder.layers.2.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>encoder.layers.2.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.942205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>encoder.layers.2.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>encoder.layers.2.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>encoder.layers.2.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>encoder.layers.2.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>encoder.layers.2.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.853813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>encoder.layers.2.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>encoder.layers.3.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>encoder.layers.3.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>encoder.layers.3.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>encoder.layers.3.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>encoder.layers.3.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>encoder.layers.3.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>encoder.layers.3.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.860612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>encoder.layers.3.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>encoder.layers.3.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>encoder.layers.3.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>encoder.layers.3.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>encoder.layers.3.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.930026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>encoder.layers.3.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>encoder.layers.3.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.941234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>encoder.layers.3.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.883924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>encoder.layers.3.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.952404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>decoder.trg_embedding.weight</td>\n",
       "      <td>(52, 512)</td>\n",
       "      <td>106496</td>\n",
       "      <td>0.930693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>decoder.positional_encoding.pe</td>\n",
       "      <td>(1, 1000, 512)</td>\n",
       "      <td>2048000</td>\n",
       "      <td>0.910408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>decoder.layers.0.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.929534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>decoder.layers.0.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>decoder.layers.0.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>decoder.layers.0.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.940748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>decoder.layers.0.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.927985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>decoder.layers.0.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>decoder.layers.0.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.820787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>decoder.layers.0.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>decoder.layers.0.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>decoder.layers.0.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>decoder.layers.0.encoder_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>decoder.layers.0.encoder_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>decoder.layers.0.encoder_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>decoder.layers.0.encoder_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>decoder.layers.0.encoder_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>decoder.layers.0.encoder_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>decoder.layers.0.encoder_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.830015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>decoder.layers.0.encoder_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.941719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>decoder.layers.0.encoder_attn.output_linear.we...</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>decoder.layers.0.encoder_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>decoder.layers.0.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>decoder.layers.0.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.922102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>decoder.layers.0.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>decoder.layers.0.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>decoder.layers.0.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.842156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>decoder.layers.0.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>decoder.layers.1.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>decoder.layers.1.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>decoder.layers.1.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>decoder.layers.1.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>decoder.layers.1.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>decoder.layers.1.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.951433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>decoder.layers.1.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.824672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>decoder.layers.1.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>decoder.layers.1.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>decoder.layers.1.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>decoder.layers.1.encoder_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>decoder.layers.1.encoder_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.944148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>decoder.layers.1.encoder_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>decoder.layers.1.encoder_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>decoder.layers.1.encoder_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>decoder.layers.1.encoder_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>decoder.layers.1.encoder_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.831472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>decoder.layers.1.encoder_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>decoder.layers.1.encoder_attn.output_linear.we...</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>decoder.layers.1.encoder_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>decoder.layers.1.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>decoder.layers.1.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.917469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>decoder.layers.1.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>decoder.layers.1.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>decoder.layers.1.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.853327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>decoder.layers.1.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>decoder.layers.2.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>decoder.layers.2.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>decoder.layers.2.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>decoder.layers.2.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>decoder.layers.2.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>decoder.layers.2.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.950461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>decoder.layers.2.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.829529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>decoder.layers.2.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>decoder.layers.2.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>decoder.layers.2.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>decoder.layers.2.encoder_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>decoder.layers.2.encoder_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>decoder.layers.2.encoder_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.924846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>decoder.layers.2.encoder_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>decoder.layers.2.encoder_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>decoder.layers.2.encoder_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>decoder.layers.2.encoder_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.839728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>decoder.layers.2.encoder_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.941719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>decoder.layers.2.encoder_attn.output_linear.we...</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>decoder.layers.2.encoder_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>decoder.layers.2.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>decoder.layers.2.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.915884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>decoder.layers.2.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>decoder.layers.2.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.946576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>decoder.layers.2.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.858184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>decoder.layers.2.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.945605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>decoder.layers.3.self_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>decoder.layers.3.self_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.942691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>decoder.layers.3.self_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>decoder.layers.3.self_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.942205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>decoder.layers.3.self_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>decoder.layers.3.self_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>decoder.layers.3.self_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.831957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>decoder.layers.3.self_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>decoder.layers.3.self_attn.output_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>decoder.layers.3.self_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>decoder.layers.3.encoder_attn.query_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>decoder.layers.3.encoder_attn.query_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.949490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>decoder.layers.3.encoder_attn.key_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.925090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>decoder.layers.3.encoder_attn.key_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>decoder.layers.3.encoder_attn.value_linear.weight</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.926811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>decoder.layers.3.encoder_attn.value_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>decoder.layers.3.encoder_attn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.842642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>decoder.layers.3.encoder_attn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>decoder.layers.3.encoder_attn.output_linear.we...</td>\n",
       "      <td>(512, 512)</td>\n",
       "      <td>1048576</td>\n",
       "      <td>0.927219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>decoder.layers.3.encoder_attn.output_linear.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.942691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>decoder.layers.3.pos_ffn.conv_1.weight</td>\n",
       "      <td>(2048, 512, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.929491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>decoder.layers.3.pos_ffn.conv_1.bias</td>\n",
       "      <td>(2048,)</td>\n",
       "      <td>8192</td>\n",
       "      <td>0.927222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>decoder.layers.3.pos_ffn.conv_2.weight</td>\n",
       "      <td>(512, 2048, 1)</td>\n",
       "      <td>4194304</td>\n",
       "      <td>0.931948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>decoder.layers.3.pos_ffn.conv_2.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.947062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>decoder.layers.3.pos_ffn.layer_norm.weight</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.855755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>decoder.layers.3.pos_ffn.layer_norm.bias</td>\n",
       "      <td>(512,)</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.948033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>decoder.output_linear.weight</td>\n",
       "      <td>(52, 512)</td>\n",
       "      <td>106496</td>\n",
       "      <td>0.931707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>conv.0.weight</td>\n",
       "      <td>(64, 1, 3, 3)</td>\n",
       "      <td>2304</td>\n",
       "      <td>0.936501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>conv.0.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>conv.2.weight</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>147456</td>\n",
       "      <td>0.925649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>conv.2.bias</td>\n",
       "      <td>(64,)</td>\n",
       "      <td>256</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>conv.5.weight</td>\n",
       "      <td>(128, 64, 3, 3)</td>\n",
       "      <td>294912</td>\n",
       "      <td>0.928030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>conv.5.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>512</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>conv.7.weight</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>589824</td>\n",
       "      <td>0.929304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>conv.7.bias</td>\n",
       "      <td>(128,)</td>\n",
       "      <td>512</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Parameter             Shape  \\\n",
       "0                          encoder.input_linear.weight       (512, 5120)   \n",
       "1                            encoder.input_linear.bias            (512,)   \n",
       "2                      encoder.layer_norm_input.weight            (512,)   \n",
       "3                        encoder.layer_norm_input.bias            (512,)   \n",
       "4                       encoder.positional_encoding.pe    (1, 4000, 512)   \n",
       "5       encoder.layers.0.self_attn.query_linear.weight        (512, 512)   \n",
       "6         encoder.layers.0.self_attn.query_linear.bias            (512,)   \n",
       "7         encoder.layers.0.self_attn.key_linear.weight        (512, 512)   \n",
       "8           encoder.layers.0.self_attn.key_linear.bias            (512,)   \n",
       "9       encoder.layers.0.self_attn.value_linear.weight        (512, 512)   \n",
       "10        encoder.layers.0.self_attn.value_linear.bias            (512,)   \n",
       "11        encoder.layers.0.self_attn.layer_norm.weight            (512,)   \n",
       "12          encoder.layers.0.self_attn.layer_norm.bias            (512,)   \n",
       "13     encoder.layers.0.self_attn.output_linear.weight        (512, 512)   \n",
       "14       encoder.layers.0.self_attn.output_linear.bias            (512,)   \n",
       "15              encoder.layers.0.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "16                encoder.layers.0.pos_ffn.conv_1.bias           (2048,)   \n",
       "17              encoder.layers.0.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "18                encoder.layers.0.pos_ffn.conv_2.bias            (512,)   \n",
       "19          encoder.layers.0.pos_ffn.layer_norm.weight            (512,)   \n",
       "20            encoder.layers.0.pos_ffn.layer_norm.bias            (512,)   \n",
       "21      encoder.layers.1.self_attn.query_linear.weight        (512, 512)   \n",
       "22        encoder.layers.1.self_attn.query_linear.bias            (512,)   \n",
       "23        encoder.layers.1.self_attn.key_linear.weight        (512, 512)   \n",
       "24          encoder.layers.1.self_attn.key_linear.bias            (512,)   \n",
       "25      encoder.layers.1.self_attn.value_linear.weight        (512, 512)   \n",
       "26        encoder.layers.1.self_attn.value_linear.bias            (512,)   \n",
       "27        encoder.layers.1.self_attn.layer_norm.weight            (512,)   \n",
       "28          encoder.layers.1.self_attn.layer_norm.bias            (512,)   \n",
       "29     encoder.layers.1.self_attn.output_linear.weight        (512, 512)   \n",
       "30       encoder.layers.1.self_attn.output_linear.bias            (512,)   \n",
       "31              encoder.layers.1.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "32                encoder.layers.1.pos_ffn.conv_1.bias           (2048,)   \n",
       "33              encoder.layers.1.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "34                encoder.layers.1.pos_ffn.conv_2.bias            (512,)   \n",
       "35          encoder.layers.1.pos_ffn.layer_norm.weight            (512,)   \n",
       "36            encoder.layers.1.pos_ffn.layer_norm.bias            (512,)   \n",
       "37      encoder.layers.2.self_attn.query_linear.weight        (512, 512)   \n",
       "38        encoder.layers.2.self_attn.query_linear.bias            (512,)   \n",
       "39        encoder.layers.2.self_attn.key_linear.weight        (512, 512)   \n",
       "40          encoder.layers.2.self_attn.key_linear.bias            (512,)   \n",
       "41      encoder.layers.2.self_attn.value_linear.weight        (512, 512)   \n",
       "42        encoder.layers.2.self_attn.value_linear.bias            (512,)   \n",
       "43        encoder.layers.2.self_attn.layer_norm.weight            (512,)   \n",
       "44          encoder.layers.2.self_attn.layer_norm.bias            (512,)   \n",
       "45     encoder.layers.2.self_attn.output_linear.weight        (512, 512)   \n",
       "46       encoder.layers.2.self_attn.output_linear.bias            (512,)   \n",
       "47              encoder.layers.2.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "48                encoder.layers.2.pos_ffn.conv_1.bias           (2048,)   \n",
       "49              encoder.layers.2.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "50                encoder.layers.2.pos_ffn.conv_2.bias            (512,)   \n",
       "51          encoder.layers.2.pos_ffn.layer_norm.weight            (512,)   \n",
       "52            encoder.layers.2.pos_ffn.layer_norm.bias            (512,)   \n",
       "53      encoder.layers.3.self_attn.query_linear.weight        (512, 512)   \n",
       "54        encoder.layers.3.self_attn.query_linear.bias            (512,)   \n",
       "55        encoder.layers.3.self_attn.key_linear.weight        (512, 512)   \n",
       "56          encoder.layers.3.self_attn.key_linear.bias            (512,)   \n",
       "57      encoder.layers.3.self_attn.value_linear.weight        (512, 512)   \n",
       "58        encoder.layers.3.self_attn.value_linear.bias            (512,)   \n",
       "59        encoder.layers.3.self_attn.layer_norm.weight            (512,)   \n",
       "60          encoder.layers.3.self_attn.layer_norm.bias            (512,)   \n",
       "61     encoder.layers.3.self_attn.output_linear.weight        (512, 512)   \n",
       "62       encoder.layers.3.self_attn.output_linear.bias            (512,)   \n",
       "63              encoder.layers.3.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "64                encoder.layers.3.pos_ffn.conv_1.bias           (2048,)   \n",
       "65              encoder.layers.3.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "66                encoder.layers.3.pos_ffn.conv_2.bias            (512,)   \n",
       "67          encoder.layers.3.pos_ffn.layer_norm.weight            (512,)   \n",
       "68            encoder.layers.3.pos_ffn.layer_norm.bias            (512,)   \n",
       "69                        decoder.trg_embedding.weight         (52, 512)   \n",
       "70                      decoder.positional_encoding.pe    (1, 1000, 512)   \n",
       "71      decoder.layers.0.self_attn.query_linear.weight        (512, 512)   \n",
       "72        decoder.layers.0.self_attn.query_linear.bias            (512,)   \n",
       "73        decoder.layers.0.self_attn.key_linear.weight        (512, 512)   \n",
       "74          decoder.layers.0.self_attn.key_linear.bias            (512,)   \n",
       "75      decoder.layers.0.self_attn.value_linear.weight        (512, 512)   \n",
       "76        decoder.layers.0.self_attn.value_linear.bias            (512,)   \n",
       "77        decoder.layers.0.self_attn.layer_norm.weight            (512,)   \n",
       "78          decoder.layers.0.self_attn.layer_norm.bias            (512,)   \n",
       "79     decoder.layers.0.self_attn.output_linear.weight        (512, 512)   \n",
       "80       decoder.layers.0.self_attn.output_linear.bias            (512,)   \n",
       "81   decoder.layers.0.encoder_attn.query_linear.weight        (512, 512)   \n",
       "82     decoder.layers.0.encoder_attn.query_linear.bias            (512,)   \n",
       "83     decoder.layers.0.encoder_attn.key_linear.weight        (512, 512)   \n",
       "84       decoder.layers.0.encoder_attn.key_linear.bias            (512,)   \n",
       "85   decoder.layers.0.encoder_attn.value_linear.weight        (512, 512)   \n",
       "86     decoder.layers.0.encoder_attn.value_linear.bias            (512,)   \n",
       "87     decoder.layers.0.encoder_attn.layer_norm.weight            (512,)   \n",
       "88       decoder.layers.0.encoder_attn.layer_norm.bias            (512,)   \n",
       "89   decoder.layers.0.encoder_attn.output_linear.we...        (512, 512)   \n",
       "90    decoder.layers.0.encoder_attn.output_linear.bias            (512,)   \n",
       "91              decoder.layers.0.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "92                decoder.layers.0.pos_ffn.conv_1.bias           (2048,)   \n",
       "93              decoder.layers.0.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "94                decoder.layers.0.pos_ffn.conv_2.bias            (512,)   \n",
       "95          decoder.layers.0.pos_ffn.layer_norm.weight            (512,)   \n",
       "96            decoder.layers.0.pos_ffn.layer_norm.bias            (512,)   \n",
       "97      decoder.layers.1.self_attn.query_linear.weight        (512, 512)   \n",
       "98        decoder.layers.1.self_attn.query_linear.bias            (512,)   \n",
       "99        decoder.layers.1.self_attn.key_linear.weight        (512, 512)   \n",
       "100         decoder.layers.1.self_attn.key_linear.bias            (512,)   \n",
       "101     decoder.layers.1.self_attn.value_linear.weight        (512, 512)   \n",
       "102       decoder.layers.1.self_attn.value_linear.bias            (512,)   \n",
       "103       decoder.layers.1.self_attn.layer_norm.weight            (512,)   \n",
       "104         decoder.layers.1.self_attn.layer_norm.bias            (512,)   \n",
       "105    decoder.layers.1.self_attn.output_linear.weight        (512, 512)   \n",
       "106      decoder.layers.1.self_attn.output_linear.bias            (512,)   \n",
       "107  decoder.layers.1.encoder_attn.query_linear.weight        (512, 512)   \n",
       "108    decoder.layers.1.encoder_attn.query_linear.bias            (512,)   \n",
       "109    decoder.layers.1.encoder_attn.key_linear.weight        (512, 512)   \n",
       "110      decoder.layers.1.encoder_attn.key_linear.bias            (512,)   \n",
       "111  decoder.layers.1.encoder_attn.value_linear.weight        (512, 512)   \n",
       "112    decoder.layers.1.encoder_attn.value_linear.bias            (512,)   \n",
       "113    decoder.layers.1.encoder_attn.layer_norm.weight            (512,)   \n",
       "114      decoder.layers.1.encoder_attn.layer_norm.bias            (512,)   \n",
       "115  decoder.layers.1.encoder_attn.output_linear.we...        (512, 512)   \n",
       "116   decoder.layers.1.encoder_attn.output_linear.bias            (512,)   \n",
       "117             decoder.layers.1.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "118               decoder.layers.1.pos_ffn.conv_1.bias           (2048,)   \n",
       "119             decoder.layers.1.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "120               decoder.layers.1.pos_ffn.conv_2.bias            (512,)   \n",
       "121         decoder.layers.1.pos_ffn.layer_norm.weight            (512,)   \n",
       "122           decoder.layers.1.pos_ffn.layer_norm.bias            (512,)   \n",
       "123     decoder.layers.2.self_attn.query_linear.weight        (512, 512)   \n",
       "124       decoder.layers.2.self_attn.query_linear.bias            (512,)   \n",
       "125       decoder.layers.2.self_attn.key_linear.weight        (512, 512)   \n",
       "126         decoder.layers.2.self_attn.key_linear.bias            (512,)   \n",
       "127     decoder.layers.2.self_attn.value_linear.weight        (512, 512)   \n",
       "128       decoder.layers.2.self_attn.value_linear.bias            (512,)   \n",
       "129       decoder.layers.2.self_attn.layer_norm.weight            (512,)   \n",
       "130         decoder.layers.2.self_attn.layer_norm.bias            (512,)   \n",
       "131    decoder.layers.2.self_attn.output_linear.weight        (512, 512)   \n",
       "132      decoder.layers.2.self_attn.output_linear.bias            (512,)   \n",
       "133  decoder.layers.2.encoder_attn.query_linear.weight        (512, 512)   \n",
       "134    decoder.layers.2.encoder_attn.query_linear.bias            (512,)   \n",
       "135    decoder.layers.2.encoder_attn.key_linear.weight        (512, 512)   \n",
       "136      decoder.layers.2.encoder_attn.key_linear.bias            (512,)   \n",
       "137  decoder.layers.2.encoder_attn.value_linear.weight        (512, 512)   \n",
       "138    decoder.layers.2.encoder_attn.value_linear.bias            (512,)   \n",
       "139    decoder.layers.2.encoder_attn.layer_norm.weight            (512,)   \n",
       "140      decoder.layers.2.encoder_attn.layer_norm.bias            (512,)   \n",
       "141  decoder.layers.2.encoder_attn.output_linear.we...        (512, 512)   \n",
       "142   decoder.layers.2.encoder_attn.output_linear.bias            (512,)   \n",
       "143             decoder.layers.2.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "144               decoder.layers.2.pos_ffn.conv_1.bias           (2048,)   \n",
       "145             decoder.layers.2.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "146               decoder.layers.2.pos_ffn.conv_2.bias            (512,)   \n",
       "147         decoder.layers.2.pos_ffn.layer_norm.weight            (512,)   \n",
       "148           decoder.layers.2.pos_ffn.layer_norm.bias            (512,)   \n",
       "149     decoder.layers.3.self_attn.query_linear.weight        (512, 512)   \n",
       "150       decoder.layers.3.self_attn.query_linear.bias            (512,)   \n",
       "151       decoder.layers.3.self_attn.key_linear.weight        (512, 512)   \n",
       "152         decoder.layers.3.self_attn.key_linear.bias            (512,)   \n",
       "153     decoder.layers.3.self_attn.value_linear.weight        (512, 512)   \n",
       "154       decoder.layers.3.self_attn.value_linear.bias            (512,)   \n",
       "155       decoder.layers.3.self_attn.layer_norm.weight            (512,)   \n",
       "156         decoder.layers.3.self_attn.layer_norm.bias            (512,)   \n",
       "157    decoder.layers.3.self_attn.output_linear.weight        (512, 512)   \n",
       "158      decoder.layers.3.self_attn.output_linear.bias            (512,)   \n",
       "159  decoder.layers.3.encoder_attn.query_linear.weight        (512, 512)   \n",
       "160    decoder.layers.3.encoder_attn.query_linear.bias            (512,)   \n",
       "161    decoder.layers.3.encoder_attn.key_linear.weight        (512, 512)   \n",
       "162      decoder.layers.3.encoder_attn.key_linear.bias            (512,)   \n",
       "163  decoder.layers.3.encoder_attn.value_linear.weight        (512, 512)   \n",
       "164    decoder.layers.3.encoder_attn.value_linear.bias            (512,)   \n",
       "165    decoder.layers.3.encoder_attn.layer_norm.weight            (512,)   \n",
       "166      decoder.layers.3.encoder_attn.layer_norm.bias            (512,)   \n",
       "167  decoder.layers.3.encoder_attn.output_linear.we...        (512, 512)   \n",
       "168   decoder.layers.3.encoder_attn.output_linear.bias            (512,)   \n",
       "169             decoder.layers.3.pos_ffn.conv_1.weight    (2048, 512, 1)   \n",
       "170               decoder.layers.3.pos_ffn.conv_1.bias           (2048,)   \n",
       "171             decoder.layers.3.pos_ffn.conv_2.weight    (512, 2048, 1)   \n",
       "172               decoder.layers.3.pos_ffn.conv_2.bias            (512,)   \n",
       "173         decoder.layers.3.pos_ffn.layer_norm.weight            (512,)   \n",
       "174           decoder.layers.3.pos_ffn.layer_norm.bias            (512,)   \n",
       "175                       decoder.output_linear.weight         (52, 512)   \n",
       "176                                      conv.0.weight     (64, 1, 3, 3)   \n",
       "177                                        conv.0.bias             (64,)   \n",
       "178                                      conv.2.weight    (64, 64, 3, 3)   \n",
       "179                                        conv.2.bias             (64,)   \n",
       "180                                      conv.5.weight   (128, 64, 3, 3)   \n",
       "181                                        conv.5.bias            (128,)   \n",
       "182                                      conv.7.weight  (128, 128, 3, 3)   \n",
       "183                                        conv.7.bias            (128,)   \n",
       "\n",
       "         Size  Compression Ratio  \n",
       "0    10485760           0.930760  \n",
       "1        2048           0.950461  \n",
       "2        2048           0.837785  \n",
       "3        2048           0.926178  \n",
       "4     8192000           0.914195  \n",
       "5     1048576           0.924644  \n",
       "6        2048           0.945119  \n",
       "7     1048576           0.924740  \n",
       "8        2048           0.949976  \n",
       "9     1048576           0.925194  \n",
       "10       2048           0.946090  \n",
       "11       2048           0.830015  \n",
       "12       2048           0.945605  \n",
       "13    1048576           0.924829  \n",
       "14       2048           0.943662  \n",
       "15    4194304           0.929579  \n",
       "16       8192           0.929416  \n",
       "17    4194304           0.928755  \n",
       "18       2048           0.948033  \n",
       "19       2048           0.838757  \n",
       "20       2048           0.947547  \n",
       "21    1048576           0.924633  \n",
       "22       2048           0.946090  \n",
       "23    1048576           0.924649  \n",
       "24       2048           0.947062  \n",
       "25    1048576           0.924598  \n",
       "26       2048           0.942205  \n",
       "27       2048           0.842156  \n",
       "28       2048           0.946576  \n",
       "29    1048576           0.924621  \n",
       "30       2048           0.947062  \n",
       "31    4194304           0.929537  \n",
       "32       8192           0.929172  \n",
       "33    4194304           0.929105  \n",
       "34       2048           0.946090  \n",
       "35       2048           0.843613  \n",
       "36       2048           0.944148  \n",
       "37    1048576           0.924671  \n",
       "38       2048           0.945119  \n",
       "39    1048576           0.924764  \n",
       "40       2048           0.947547  \n",
       "41    1048576           0.924720  \n",
       "42       2048           0.946576  \n",
       "43       2048           0.852841  \n",
       "44       2048           0.946576  \n",
       "45    1048576           0.924665  \n",
       "46       2048           0.942205  \n",
       "47    4194304           0.929510  \n",
       "48       8192           0.927100  \n",
       "49    4194304           0.929337  \n",
       "50       2048           0.943662  \n",
       "51       2048           0.853813  \n",
       "52       2048           0.944633  \n",
       "53    1048576           0.924717  \n",
       "54       2048           0.949004  \n",
       "55    1048576           0.924958  \n",
       "56       2048           0.943176  \n",
       "57    1048576           0.924770  \n",
       "58       2048           0.944148  \n",
       "59       2048           0.860612  \n",
       "60       2048           0.944148  \n",
       "61    1048576           0.924982  \n",
       "62       2048           0.943662  \n",
       "63    4194304           0.929912  \n",
       "64       8192           0.930026  \n",
       "65    4194304           0.929705  \n",
       "66       2048           0.941234  \n",
       "67       2048           0.883924  \n",
       "68       2048           0.952404  \n",
       "69     106496           0.930693  \n",
       "70    2048000           0.910408  \n",
       "71    1048576           0.929534  \n",
       "72       2048           0.949004  \n",
       "73    1048576           0.926500  \n",
       "74       2048           0.940748  \n",
       "75    1048576           0.927985  \n",
       "76       2048           0.944633  \n",
       "77       2048           0.820787  \n",
       "78       2048           0.949004  \n",
       "79    1048576           0.924769  \n",
       "80       2048           0.949004  \n",
       "81    1048576           0.924807  \n",
       "82       2048           0.945119  \n",
       "83    1048576           0.925127  \n",
       "84       2048           0.948519  \n",
       "85    1048576           0.926383  \n",
       "86       2048           0.949490  \n",
       "87       2048           0.830015  \n",
       "88       2048           0.941719  \n",
       "89    1048576           0.925709  \n",
       "90       2048           0.944633  \n",
       "91    4194304           0.929581  \n",
       "92       8192           0.922102  \n",
       "93    4194304           0.929344  \n",
       "94       2048           0.944633  \n",
       "95       2048           0.842156  \n",
       "96       2048           0.949490  \n",
       "97    1048576           0.924756  \n",
       "98       2048           0.945605  \n",
       "99    1048576           0.924807  \n",
       "100      2048           0.944633  \n",
       "101   1048576           0.926119  \n",
       "102      2048           0.951433  \n",
       "103      2048           0.824672  \n",
       "104      2048           0.946090  \n",
       "105   1048576           0.926346  \n",
       "106      2048           0.947062  \n",
       "107   1048576           0.924695  \n",
       "108      2048           0.944148  \n",
       "109   1048576           0.925076  \n",
       "110      2048           0.949976  \n",
       "111   1048576           0.926169  \n",
       "112      2048           0.945605  \n",
       "113      2048           0.831472  \n",
       "114      2048           0.947547  \n",
       "115   1048576           0.925841  \n",
       "116      2048           0.946090  \n",
       "117   4194304           0.929627  \n",
       "118      8192           0.917469  \n",
       "119   4194304           0.929405  \n",
       "120      2048           0.946576  \n",
       "121      2048           0.853327  \n",
       "122      2048           0.949004  \n",
       "123   1048576           0.925298  \n",
       "124      2048           0.948033  \n",
       "125   1048576           0.925314  \n",
       "126      2048           0.943176  \n",
       "127   1048576           0.925843  \n",
       "128      2048           0.950461  \n",
       "129      2048           0.829529  \n",
       "130      2048           0.945605  \n",
       "131   1048576           0.926106  \n",
       "132      2048           0.946090  \n",
       "133   1048576           0.924715  \n",
       "134      2048           0.943662  \n",
       "135   1048576           0.924846  \n",
       "136      2048           0.949490  \n",
       "137   1048576           0.926433  \n",
       "138      2048           0.946576  \n",
       "139      2048           0.839728  \n",
       "140      2048           0.941719  \n",
       "141   1048576           0.926575  \n",
       "142      2048           0.943662  \n",
       "143   4194304           0.929711  \n",
       "144      8192           0.915884  \n",
       "145   4194304           0.929392  \n",
       "146      2048           0.946576  \n",
       "147      2048           0.858184  \n",
       "148      2048           0.945605  \n",
       "149   1048576           0.925487  \n",
       "150      2048           0.942691  \n",
       "151   1048576           0.925499  \n",
       "152      2048           0.942205  \n",
       "153   1048576           0.925313  \n",
       "154      2048           0.948033  \n",
       "155      2048           0.831957  \n",
       "156      2048           0.948519  \n",
       "157   1048576           0.926016  \n",
       "158      2048           0.943662  \n",
       "159   1048576           0.925340  \n",
       "160      2048           0.949490  \n",
       "161   1048576           0.925090  \n",
       "162      2048           0.947062  \n",
       "163   1048576           0.926811  \n",
       "164      2048           0.947062  \n",
       "165      2048           0.842642  \n",
       "166      2048           0.948519  \n",
       "167   1048576           0.927219  \n",
       "168      2048           0.942691  \n",
       "169   4194304           0.929491  \n",
       "170      8192           0.927222  \n",
       "171   4194304           0.931948  \n",
       "172      2048           0.947062  \n",
       "173      2048           0.855755  \n",
       "174      2048           0.948033  \n",
       "175    106496           0.931707  \n",
       "176      2304           0.936501  \n",
       "177       256           1.000000  \n",
       "178    147456           0.925649  \n",
       "179       256           1.000000  \n",
       "180    294912           0.928030  \n",
       "181       512           1.000000  \n",
       "182    589824           0.929304  \n",
       "183       512           1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139.683584"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compression['Size'].sum()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36864"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "147456/36864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
