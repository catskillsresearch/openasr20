{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript to BUILD wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from matplotlib.pylab import *\n",
    "import librosa\n",
    "import torch\n",
    "from epoch_time import epoch_time\n",
    "from tqdm.notebook import tqdm\n",
    "from txt_to_stm import txt_to_stm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from padarray import padarray\n",
    "from to_samples import to_samples\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import audioread\n",
    "import random\n",
    "import soundfile as sf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/catskills/Desktop/openasr20'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage='NIST'\n",
    "sample_rate=8000\n",
    "window = sample_rate\n",
    "H=window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = list(sorted(glob(f'{stage}/*/build/transcription/*.txt')))\n",
    "len(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files=[x.replace('/transcription/', '/audio/').replace('.txt','.wav') for x in transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d912e6a69f44b3b70d16352cd82925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=122.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for transcript_file in tqdm(transcripts):\n",
    "    audio_file = transcript_file.replace('/transcription/', '/audio/').replace('.txt','.wav')\n",
    "    if not os.path.exists(audio_file):\n",
    "        print('missing', audio_file)\n",
    "        continue\n",
    "        \n",
    "    # Create split dirs\n",
    "    audio_dir=os.path.dirname(audio_file)\n",
    "    audio_split_dir=audio_dir.replace('/audio', '/audio_split')\n",
    "    Path(audio_split_dir).mkdir(parents=True, exist_ok=True)\n",
    "    transcript_dir=os.path.dirname(transcript_file)\n",
    "    transcript_split_dir=transcript_dir.replace('/transcription', '/transcription_split')\n",
    "    Path(transcript_split_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load audio\n",
    "    file = \"_\".join(os.path.basename(transcript_file).split(\"_\")[:-1])\n",
    "    channel = os.path.basename(transcript_file).split(\"_\")[-1].split(\".\")[-2]\n",
    "    transcript_df = pd.read_csv(transcript_file, sep = \"\\n\", header = None, names = [\"content\"])\n",
    "    result = txt_to_stm(transcript_df, file, channel)\n",
    "    speech=[(float(x[-3]), float(x[-2]), x[-1]) for x in result if len(x)==6]\n",
    "    x_np,sr=librosa.load(audio_file, sr=sample_rate)\n",
    "    with audioread.audio_open(audio_file) as f:\n",
    "        sr = f.samplerate\n",
    "    if sr != sample_rate:\n",
    "        print('RESIZING', sr, audio_file)\n",
    "        sf.write(audio_file, x_np, sample_rate)\n",
    "        \n",
    "    # Split audio\n",
    "    speech_segments=[(int(a*sample_rate), int(b*sample_rate), words) for (a,b,words) in speech]\n",
    "    for i, (lower, upper, words) in enumerate(speech_segments):\n",
    "        audio_split_file=f\"{audio_file[0:-4].replace('/audio/','/audio_split/')}_{i:03d}.wav\"\n",
    "        sf.write(audio_split_file, x_np[lower:upper], sample_rate)\n",
    "\n",
    "        transcript_split_file=f\"{transcript_file[0:-4].replace('/transcription/','/transcription_split/')}_{i:03d}.txt\"\n",
    "        with open(transcript_split_file,'w') as f:\n",
    "            f.write(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openasr",
   "language": "python",
   "name": "openasr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
